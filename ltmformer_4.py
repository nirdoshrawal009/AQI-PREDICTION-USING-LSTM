# -*- coding: utf-8 -*-
"""LTMFormer_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u3Aj0cmCZnPBqNUGL27bKyC2rAjiiobO
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import time
import functools

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, BatchSampler
from torch.utils.data.dataset import Dataset
import torch.optim as optim
from sklearn.model_selection import KFold
import torch.utils.data as data_utils

from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_log_error as msle
from sklearn.model_selection import StratifiedKFold, KFold
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# %matplotlib inline

DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

df1 = pd.read_csv('/content/Jodhpur_1.csv')
df1 = df1[['PM2.5','PM10','SO2','NO2','CO','O3']]
df1.head()

df2 = pd.read_csv('/content/Jodhpur_2.csv')
df2 = df2[['PM2.5','PM10','SO2','NO2','CO','O3']]
df3 = pd.read_csv('/content/Jodhpur_3.csv')
df3 = df3[['PM2.5','PM10','SO2','NO2','CO','O3']]
df4 = pd.read_csv('/content/Jodhpur_4.csv')
df4 = df4[['PM2.5','PM10','SO2','NO2','CO','O3']]

final_df = pd.concat([df1, df2, df3, df4], axis=0)

final_df.shape

class Trans:
    def __init__(self, data, name):
        self.min = max(0, np.percentile(data, 1))
        self.max = np.percentile(data, 99)
        self.base = self.max-self.min

    def transform(self, data, scale=True):
        _data = np.clip(data, self.min, self.max)
        if not scale:
            return _data
        return (_data-self.min)/self.base

class TransUtil:
    def __init__(self, data, exclude_cols=None):
        self.columns = data.columns
        self.exclude_cols = exclude_cols
        self.trans = {}
        for c in self.columns:
            if data[c].dtype not in [int, float]:
                print('column "{}" not init trans...'.format(c))
                continue

            if exclude_cols is None or (exclude_cols is not None and c not in exclude_cols):
                print('init trans column...', c)
                self.trans[c] = Trans(data[c].fillna(method='backfill').fillna(method='ffill'), c)

    def transform(self, data, col_name, scale=True):
        if self.exclude_cols is not None and col_name in self.exclude_cols:
            return data

        for t in self.trans:
            if t.startswith(col_name):
                return self.trans[t].transform(data, scale=scale)
        
        return data

trans_util = TransUtil(final_df, exclude_cols=None) # data standardization

def generate_xy_pair(final_df, seq_len, trans_util, columns_x, columns_y):
    data_x = pd.DataFrame()
    for c in columns_x:
        data_x[c] = trans_util.transform(final_df[c].fillna(final_df[c].median()), c)

    data_y = pd.DataFrame()
    for c in columns_y:
        data_y[c] = trans_util.transform(final_df[c].fillna(final_df[c].median()), c, scale=False)

    data_x = data_x.values
    data_y = data_y.values
    
    print(data_x.shape, data_y.shape)

    d_x = []
    d_y = []
    for i in range(len(data_x)-seq_len*2+1):
        _x = data_x[i:i+seq_len]
        _y = data_y[i+seq_len:i+seq_len+seq_len]

        assert len(_x) == len(_y) == seq_len, (_x, _y, _x.shape, _y.shape, i, len(data_x))

        d_x.append(_x.T)
        d_y.append(_y.T)

    return np.asarray(d_x).transpose((0, 2, 1)), np.asarray(d_y).transpose((0, 2, 1))

SEQ_LEN =168

COLUMNS_Y = (["PM2.5","PM10",	"SO2", "NO2",	"CO",	"O3"])
COLUMNS_X = COLUMNS_Y
COLUMNS_X, COLUMNS_Y

data_x, data_y = generate_xy_pair(final_df, seq_len=SEQ_LEN, trans_util=trans_util, columns_x=COLUMNS_Y, columns_y=COLUMNS_Y)

data_x.shape, data_y.shape

data_x[0], data_y[0]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.33, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

X_train[0].shape

X_train = np.array(X_train, dtype=np.float)
y_train = np.array(y_train, dtype=np.float)
X_test = np.array(X_test, dtype=np.float)
y_test = np.array(y_test, dtype=np.float)

X_train[0], X_test[1], y_train[0], y_test[1]

class Tt(nn.Module):
    def __init__(self,
                 seq_len,
                 feature_size,
                 output_size,
                 device,
                 use_model='lstm',
                 hidden_size=576,
                 num_hidden_layers=6,
                 num_attention_heads=6,
                 intermediate_size=3072,
                 hidden_act="gelu",
                 hidden_dropout_prob=0.1,
                 attention_probs_dropout_prob=0.1,
                 max_position_embeddings=512,
                #  max_1=25,
                #  max_2=61,
                #  max_3=8,
                #  max_4=1441
                 ):
        super(Tt, self).__init__()

        self.device = device
        self.use_model = use_model
        self.feature_size = feature_size

        # If there is a corresponding time embedding, it can be used
        # self.th_embeddings = nn.Embedding(max_hour, hidden_size)
        # self.tm_embeddings = nn.Embedding(max_min, hidden_size)
        # self.td_embeddings = nn.Embedding(max_dow, hidden_size)
        # self.tt_embeddings = nn.Embedding(max_ts, hidden_size)

        # location code
        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size).to(self.device)
        self.layer_norm = nn.LayerNorm(hidden_size).to(self.device)
        self.fc_inputs = nn.Linear(feature_size, hidden_size).to(self.device)

        encoder_layer = nn.TransformerEncoderLayer(
            hidden_size,
            num_attention_heads,
            intermediate_size,
            dropout=hidden_dropout_prob,
            activation=hidden_act)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_hidden_layers).to(self.device)

        self.lstm = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=2).to(self.device)

        self.fc_output_1 = nn.Linear(hidden_size, hidden_size).to(self.device)
        self.fc_output_2 = nn.Linear(hidden_size, hidden_size).to(self.device)
        self.fc_output_3 = nn.Linear(hidden_size, output_size).to(self.device)

    def forward(self,
                inputs,
                # inputs_th=None,
                # inputs_tm=None,
                # inputs_td=None,
                # inputs_tt=None,
                position_ids=None,
                attention_mask=None):

        if position_ids is None:
            # print(inputs.shape[:2])
            ones = torch.ones(inputs.size()[:2], dtype=torch.long, device=self.device)
            seq_length = torch.cumsum(ones, axis=1)
            # seq_length = torch.mean(seq_length, axis=1)
            position_ids = seq_length - ones
            position_ids.stop_gradient = True
        
        # print("positionids",position_ids.size())
        position_embeddings = self.position_embeddings(position_ids)

        # print(self.fc_inputs.weight.dtype)
        inputs = self.fc_inputs(inputs)
        inputs = nn.Tanh()(inputs)

        #print(position_embeddings.size())
        inputs = inputs + position_embeddings

        # # If there is a corresponding time embedding, it can be used
        # if inputs_th is not None:
        #     inputs += self.th_embeddings(inputs_th)
        
        # if inputs_tm is not None:
        #     inputs += self.tm_embeddings(inputs_tm)

        # if inputs_td is not None:
        #     inputs += self.td_embeddings(inputs_td)

        # if inputs_tt is not None:
        #     inputs += self.tt_embeddings(inputs_tt)

        inputs = self.layer_norm(inputs)

        # Choose to use LSTM or Transformer
        if self.use_model == 'lstm':
            encoder_outputs, (h, c) = self.lstm(inputs)
        elif self.use_model == 'transformer':
            if attention_mask is None:
                attention_mask = torch.unsqueeze(
                    (torch.zeros(inputs.shape[:2])).astype(
                        self.fc_inputs.weight.dtype) * -1e4,
                    axis=[1, 2])

            encoder_outputs = self.encoder(
                inputs,
                src_mask=attention_mask)

        output = self.fc_output_1(encoder_outputs)
        output = nn.ReLU()(output)
        output = self.fc_output_2(output)
        output = self.fc_output_3(output)

        return output

SEQ_LEN = 168
FEATURE_SIZE = 6
OUTPUT_SIZE = 6
model = Tt(seq_len=SEQ_LEN, feature_size=FEATURE_SIZE, output_size=OUTPUT_SIZE, device=DEVICE)

x = torch.randn((64, 168, 6))
# x_numpy = x.detach().numpy()
y = model(inputs=x.to(DEVICE))
y.size()

# xt = torch.ones(x_numpy.shape[:2], dtype=torch.long)
y.size()

def calc_score(y_true, y_pred):
    y_true = np.nan_to_num(y_true)
    y_pred = np.nan_to_num(y_pred)
    return 1/ (1+msle(np.clip(np.reshape(y_true, -1), 0, None), np.clip(np.reshape(y_pred, -1), 0, None)))

def eval_model(model, data_loader):
    model.eval()

    y_pred = []
    y_true = []
    for step, (data, label) in enumerate(data_loader, start=1):
        # print(batch)
        # print(batch[0].shape)
        data = data.to(torch.float32).to(DEVICE)
        label = label.to(torch.float32).to(DEVICE)

        # Computational model output
        output = model(inputs=data)
        y_pred.extend(output.cpu().detach().numpy())
        y_true.extend(label.cpu().detach().numpy())
    
    score = calc_score(y_true, y_pred)
    model.train()
    return score

# def make_data_loader(data_x, idx, batch_size, data_y=None, shuffle=False):
#     data = [{'data': torch.Tensor(data_x[i]), 'label': 0 if data_y is None else data_y[i]} for i in idx]
#     ds = MapDataset(data)
#     batch_sampler = BatchSampler(ds, batch_size=batch_size, shuffle=shuffle)
#     return DataLoader(dataset=ds, batch_sampler=batch_sampler)

from torch.utils.data import Dataset
import torchvision.transforms as T

class CustomDataset(Dataset):
  def __init__(self, X=X_train, y=y_train):
    self.X = X
    self.y = y

  def __len__(self):
    return len(self.X)

  def __getitem__(self, index):
      return torch.tensor(self.X[index], requires_grad=True).type(torch.float), torch.tensor(self.y[index], requires_grad=True).type(torch.float)

len(y_train)

dataset = CustomDataset()
x, y = dataset[0]
x

# Commented out IPython magic to ensure Python compatibility.
EPOCHS = 10 # 30 epochs they used
BATCH_SIZE = 256
CKPT_DIR = 'work/output'
K_FOLD = 5
epoch_base = 0
step_eval = 1
step_log = 1

def do_train(X_train, y_train, prefix):
    print('-'*5)
    print('training ...', prefix)
    print('train x:', X_train.shape, 'train y:', y_train.shape)

    torch.manual_seed(2022)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    for kfold, tv_idx in enumerate(KFold(n_splits=K_FOLD, shuffle=True, random_state=2022).split(X_train)):
        print('training fold...', kfold)

        train_idx, valid_idx = tv_idx

        model = Tt(seq_len=SEQ_LEN, feature_size=FEATURE_SIZE, output_size=OUTPUT_SIZE, device=DEVICE)
        model = model.to(device)

        # Assuming BATCH_SIZE is defined somewhere else in the code
        train_dataset = CustomDataset(X_train, y_train)
        valid_dataset = CustomDataset(X_test, y_test)        

        # train_dataset = MyDataset(train_x[train_idx], train_y[train_idx])
        # valid_dataset = MyDataset(train_x[valid_idx], train_y[valid_idx])

        train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        valid_data_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)

        optimizer = optim.AdamW(model.parameters(), lr=1e-4)
        criterion = nn.MSELoss()

        epochs = EPOCHS # training rounds
        save_dir = CKPT_DIR #Folder to save model parameters during training
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        global_step = 0 #iterations
        tic_train = time.time()

        model.train()

        best_score = 0
        for epoch in range(1+epoch_base, epochs+epoch_base+1):
            for step, (data, label) in enumerate(train_data_loader, start=1):
                data = data.to(torch.float32).to(device)
                label = label.to(device)

                # Computational model output
                # print(data.dtype)
                output = model(inputs=data)
                loss = criterion(output, label)
                # print(loss)

                # Print loss function value, accuracy rate, calculation speed
                global_step += 1
                if global_step % step_eval == 0:
                    score = eval_model(model, valid_data_loader)            
                    if score > best_score:
                        # print('saving best model...', score)
                        save_path = os.path.join(save_dir, f'{prefix}_kfold_{kfold}_best_model.pth')
                        torch.save(model.state_dict(), save_path)
                        best_score = score
                    if global_step % step_log == 0:
                        print(
                            'global step %d, epoch: %d, batch: %d, loss: %.5f, valid score: %.5f, speed: %.2f step/s'
#                             % (global_step, epoch, step, loss.item(), score,
                                10 / (time.time() - tic_train)))
                        tic_train = time.time()

                # Reverse gradient return, update parameters
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

# class Tt(nn.Module):
#     def __init__(self, seq_len, feature_size, output_size):
#         super(Tt, self).__init__()
#         self.seq_len = seq_len
#         self.feature_size = feature_size
#         self.output_size = output_size

#         self.fc1 = nn.Linear(seq_len * feature_size, 512)
#         self.fc2 = nn.Linear(512, 256)
#         self.fc3 = nn.Linear(256, output_size)

#     def forward(self, inputs):
#         x = inputs.view(-1, self.seq_len * self.feature_size)

def do_pred(test_x, prefix):
    print('-'*6)
    print('predict ...', prefix)
    print('predict x:', test_x.shape)

    # predict
    test_dataset = CustomDataset(X_test,y_test)
    test_data_loader = DataLoader(test_dataset, BATCH_SIZE, pin_memory=True, num_workers = 2)

    sub_df = []
    save_dir = CKPT_DIR

    for kfold in range(K_FOLD):
        print('predict kfold...', kfold)
        model = Tt(seq_len=SEQ_LEN, feature_size=FEATURE_SIZE, output_size=OUTPUT_SIZE, device=DEVICE)
        model.load_state_dict(torch.load(os.path.join(save_dir, '{}_kfold_{}_best_model.pth'.format(prefix, kfold))))
        model = model.to(DEVICE)
        model.eval()

        y_pred = []
        with torch.no_grad():
            for step, (data, label) in enumerate(test_data_loader, start=1):
                data = data.to(torch.float).to(DEVICE)
                label = label.to(torch.float).to(DEVICE)

                # Computational model output
                output = model(inputs=data)
                y_pred.extend(output.cpu().numpy())

        sub_df.append(np.clip(y_pred, 0, None))

    return sub_df

# Train the model corresponding to each test set in turn
do_train(X_train, y_train, 'm1')

pred_1 = do_pred(X_test, 'm1')

pred_1

